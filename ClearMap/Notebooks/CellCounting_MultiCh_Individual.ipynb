{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "os.environ['DISPLAY'] = ':999'\n",
    "sys.path.append('/home/zachpen87/clearmap/ClearMap2/')\n",
    "\n",
    "import h5py\n",
    "import holoviews as hv\n",
    "import dask.array as da\n",
    "import scipy.ndimage as ndimage\n",
    "import skimage\n",
    "from ClearMap.Environment import * \n",
    "import ClearMap.ImageProcessing.H5 as H5img\n",
    "hv.notebook_extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify directory information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel             = 'fos_790'\n",
    "directory           = '/home/zachpen87/clearmap/data/SEFL17b/ms_53/' \n",
    "hdf5_file           = os.path.join(directory,'data.hdf5')\n",
    "coords_file         = os.path.join(directory,'coords.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import ClearMap2 Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = wsp.Workspace('CellMap', directory=directory);\n",
    "resources_directory = settings.resources_path\n",
    "\n",
    "with h5py.File(hdf5_file,'a') as f:\n",
    "    print('hdf5_file datasets: {x}'.format(x=list(f.keys())))\n",
    "    print('hdf5_file shape: {x}\\n'.format(x=f[channel].shape))\n",
    "print(ws.info())\n",
    "\n",
    "coords = np.load(coords_file)\n",
    "print('slice coordinates: y={y}, x={x}'.format(y=coords[0],x=coords[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%output size=80\n",
    "test_data={}\n",
    "\n",
    "with h5py.File(os.path.join(directory,'data.hdf5'),'r') as f:\n",
    "    print(f[channel].shape)\n",
    "    test_data['raw'] = f[channel][900:930,1500:2000,200:700]\n",
    "    \n",
    "    \n",
    "#smooth with median filter\n",
    "#as this filter is to remove high frequency, granular noise, ksize of (3,3,3) is often good\n",
    "test_data['mfilter'] = H5img.array_filter(\n",
    "    test_data['raw'], \n",
    "    filt='median', \n",
    "    ksize=(3,3,3))\n",
    "\n",
    "\n",
    "#estimate background with morphological opening\n",
    "#kernel size should be at least diameter of larger cells in all dimensions\n",
    "test_data['bg']  = H5img.array_filter(\n",
    "    test_data['mfilter'],\n",
    "    filt='opening', \n",
    "    ksize=(5,5,5))\n",
    "\n",
    "\n",
    "#remove background from median filtered iamge\n",
    "test_data['bg_rmv'] = test_data['mfilter'] - test_data['bg']\n",
    "\n",
    "\n",
    "#threshold background subtracted image to prep for subsequent steps\n",
    "t = 10\n",
    "print('threshold: {t}'.format(t=t))\n",
    "test_data['thresh'] = test_data['bg_rmv'].copy()\n",
    "test_data['thresh'][test_data['bg_rmv']<t] = 0\n",
    "\n",
    "\n",
    "#calculate distance transform\n",
    "test_data['dist'] = ndimage.distance_transform_cdt(test_data['thresh'])\n",
    "\n",
    "\n",
    "#find local max of distance transform\n",
    "test_data['lmax'] = H5img.array_filter(\n",
    "    test_data['dist'], \n",
    "    filt='local_max',\n",
    "    ksize=(10,10,10))\n",
    "\n",
    "\n",
    "#label objects and apply size restriction\n",
    "test_data['lbls'],mx = ndimage.label(test_data['lmax'], structure=np.ones((3,3,3)))\n",
    "test_data['lbls'] = H5img.droplbls_arr(test_data['lbls'], min_size = None, max_size=300)\n",
    "\n",
    "\n",
    "#find centroids\n",
    "centroids = ndimage.measurements.center_of_mass(\n",
    "    test_data['bg_rmv'], \n",
    "    labels = test_data['lbls'], \n",
    "    index = np.unique(test_data['lbls'])[np.where(np.unique(test_data['lbls'])!=0)])\n",
    "centroids = np.array([list(ctr) for ctr in centroids]).astype('int')\n",
    "print('objects found: {x}'.format(x=len(centroids)))\n",
    "\n",
    "\n",
    "#create array representing centers\n",
    "test_data['ctrs'] = np.zeros(test_data['raw'].shape)\n",
    "test_data['ctrs'][tuple(centroids.T)]=1\n",
    "test_data['ctrs'] = H5img.array_filter(test_data['ctrs'], filt='dilation',ksize=(3,3,3))\n",
    "\n",
    "\n",
    "i=2\n",
    "p='h'\n",
    "raw = jvis.gen_hmap(img=test_data['raw'],plane=p,title='raw',inter=i,tools=['hover'],lims=(50,200))\n",
    "bg = jvis.gen_hmap(img=test_data['bg'],plane=p,title='bg',inter=i,tools=['hover'])\n",
    "sig = jvis.gen_hmap(img=test_data['bg_rmv'],plane=p,title='bg removed',inter=i,cmap='viridis',tools=['hover'], lims=(0,20))\n",
    "thresh = jvis.gen_hmap(img=test_data['thresh'],plane=p,title='thresh',inter=i,cmap='viridis',tools=['hover'])\n",
    "ctrs = jvis.gen_hmap(img=test_data['ctrs'],plane=p,title='centers',inter=i,cmap='Reds',alpha=.6,tools=['hover'])\n",
    "\n",
    "#(raw + bg + sig + (thresh*ctrs).opts(title='centroids')).cols(2)\n",
    "(raw + bg + sig + thresh*ctrs).opts(title='centroids').cols(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%output size = 100\n",
    "\n",
    "#get histogram of holes\n",
    "freqs, edges = np.histogram(test_data['bg_rmv'], 500)\n",
    "hist_holes = hv.Histogram((edges, freqs)).opts(\n",
    "    xlabel = 'Intensity of Background removed image',\n",
    "    ylabel = 'log(frequency)'\n",
    ")\n",
    "hist_holes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Processing Pipeline for Single 3d Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define cropping coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slc = (slice(None,None), slice(coords[0,0],coords[0,1]), slice(coords[1,0],coords[1,1]))\n",
    "slc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = (500,500,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth image with median filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "H5img.hdf5_filter(\n",
    "    hdf5_file = hdf5_file,\n",
    "    dset_in = channel,\n",
    "    dset_out = '_'.join([channel, 'smooth']), \n",
    "    filt = 'median', ksize  = (3,3,3),\n",
    "    chunksize = chunksize,\n",
    "    slc = slc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate background with morphological opening and subtract from smoothed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "H5img.hdf5_filter(\n",
    "    hdf5_file = hdf5_file,\n",
    "    dset_in = '_'.join([channel, 'smooth']),\n",
    "    dset_out = '_'.join([channel, 'bg']),\n",
    "    filt = 'opening', ksize  = (5,5,5),\n",
    "    chunksize = chunksize)\n",
    "\n",
    "H5img.hdf5_bgsubtract(\n",
    "    hdf5_file = hdf5_file,\n",
    "    dset_img = '_'.join([channel, 'smooth']),\n",
    "    dset_bg = '_'.join([channel, 'bg']),\n",
    "    dset_out = '_'.join([channel, 'bg_rmv']),\n",
    "    chunksize = chunksize)\n",
    "\n",
    "with h5py.File(hdf5_file,'a') as f:\n",
    "    print(f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold background subtracted image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "threshold = 10\n",
    "with h5py.File(hdf5_file,'a') as f:\n",
    "    image = da.from_array(\n",
    "            f['_'.join([channel, 'bg_rmv'])],\n",
    "            chunks = chunksize) \n",
    "    image[image<threshold] = 0\n",
    "    image.to_hdf5(hdf5_file,('/'+'_'.join([channel, 'thresh'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distance transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "H5img.hdf5_filter(\n",
    "    hdf5_file = hdf5_file,\n",
    "    dset_in = '_'.join([channel, 'thresh']),\n",
    "    dset_out = '_'.join([channel, 'dist']),\n",
    "    filt = 'distance_transform',\n",
    "    chunksize = chunksize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define local maxima of distance transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "H5img.hdf5_filter(\n",
    "    hdf5_file = hdf5_file,\n",
    "    dset_in = '_'.join([channel, 'dist']),\n",
    "    dset_out = '_'.join([channel, 'local_max']),\n",
    "    filt = 'local_max',\n",
    "    ksize = (10,10,10),\n",
    "    chunksize = chunksize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label contiguous local maxima as cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "H5img.label(\n",
    "    hdf5_file = hdf5_file,\n",
    "    dset_in = '_'.join([channel, 'local_max']),\n",
    "    dset_out = '_'.join([channel, 'lbls']),\n",
    "    min_size = None, max_size=500,\n",
    "    chunk_dimension = 1,\n",
    "    chunk_size = 400,\n",
    "    chunk_overlap = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "centroids = H5img.find_centroids(\n",
    "    hdf5_file = hdf5_file,\n",
    "    dset_lbls = '_'.join([channel, 'lbls']),\n",
    "    dset_wts = '_'.join([channel, 'bg_rmv']),\n",
    "    chunk_dimension = 1,\n",
    "    chunk_size = 400,\n",
    "    chunk_overlap = 50)\n",
    "with h5py.File(hdf5_file,'a') as f:\n",
    "    centroids = centroids.astype('int')\n",
    "    if '_'.join([channel, 'centroids/original']) in f.keys():\n",
    "        del f['_'.join([channel, 'centroids/original'])]\n",
    "    f.create_dataset('_'.join([channel, 'centroids/original']), data=centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform centroids to ABA coordinate plane and define annotation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdf5_file,'a') as f:\n",
    "    centroids = f['_'.join([channel, 'centroids/original'])][:]\n",
    "    input_shape = f['_'.join([channel, 'bg_rmv'])].shape\n",
    "\n",
    "    coordinates = res.resample_points(\n",
    "                      np.flip(centroids,axis=1), sink=None, orientation=None, \n",
    "                      source_shape = tuple(np.flip(input_shape)), \n",
    "                      sink_shape = io.shape(ws.filename('resampled', postfix=channel.split('_')[1])))\n",
    "\n",
    "    coordinates = elx.transform_points(\n",
    "                      coordinates, sink=None, \n",
    "                      transform_directory=ws.filename('resampled_to_auto', postfix=channel.split('_')[1]), \n",
    "                      binary=True, indices=False);\n",
    "\n",
    "    coordinates = elx.transform_points(\n",
    "                      coordinates, sink=None, \n",
    "                      transform_directory=ws.filename('auto_to_reference'),\n",
    "                      binary=True, indices=False);\n",
    "    \n",
    "    labels = ano.label_points(\n",
    "            points =  coordinates,\n",
    "            annotation_file = '/home/zachpen87/clearmap/AtlasDocs/Horizontal/ABA_25um_annotation.tif',\n",
    "            key = 'id')\n",
    "    \n",
    "    coordinates = np.flip(coordinates, axis=1)\n",
    "    if '_'.join([channel, 'centroids/transformed']) in f.keys():\n",
    "        del f['_'.join([channel, 'centroids/transformed'])]\n",
    "    if '_'.join([channel, 'centroids/labels']) in f.keys():\n",
    "        del f['_'.join([channel, 'centroids/labels'])]\n",
    "    f.create_dataset('_'.join([channel, 'centroids/transformed']), data=coordinates)\n",
    "    f.create_dataset('_'.join([channel, 'centroids/labels']), data=labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABA_directory = '/home/zachpen87/clearmap/AtlasDocs/Horizontal'\n",
    "ABA_ref = 'ABA_25um_reference__1_2_3__slice_None_None_None__slice_None_None_None__slice_None_None_None__.tif'\n",
    "ref = skimage.io.imread(os.path.join(ABA_directory, ABA_ref))\n",
    "\n",
    "with h5py.File(hdf5_file,'a') as f:\n",
    "    heatmap = np.zeros(ref.shape)\n",
    "    coordinates = f['_'.join([channel, 'centroids/transformed'])][:]\n",
    "    for x in coordinates.astype('int'):\n",
    "        try:\n",
    "            heatmap[x[0],x[1],x[2]] += 1\n",
    "        except:\n",
    "            pass\n",
    "    heatmap = H5img.array_filter(heatmap,filt='gaussian',ksize=(2,2,2))\n",
    "    if '_'.join([channel, 'heatmap']) in f.keys():\n",
    "        del f['_'.join([channel, 'heatmap'])]\n",
    "    f.create_dataset('_'.join([channel, 'heatmap']), data = heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%output size = 150\n",
    "\n",
    "interval = 5\n",
    "plane = 'c'\n",
    "\n",
    "with h5py.File(hdf5_file,'r') as f:\n",
    "    ABA_directory = '/home/zachpen87/clearmap/AtlasDocs/Horizontal'\n",
    "    ABA_ref = 'ABA_25um_reference__1_2_3__slice_None_None_None__slice_None_None_None__slice_None_None_None__.tif'\n",
    "    ref = skimage.io.imread(os.path.join(ABA_directory, ABA_ref))\n",
    "    ref_i = jvis.gen_hmap(ref,plane=plane,title='Allen Brain Atlas',inter=interval)\n",
    "    #hmap_i = jvis.gen_hmap(f['_'.join([channel, 'heatmap'])][:],plane=plane,title='Cells',inter=interval,cmap='inferno',lims=(0,.2),alpha=.6,tools=['hover'])\n",
    "\n",
    "#(ref_i*hmap_i).opts(title='Cells mapped to Allen Brain Atlas')\n",
    "ref_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%output size = 150\n",
    "\n",
    "interval = 40\n",
    "plane = 'c'\n",
    "\n",
    "with h5py.File(hdf5_file,'r') as f:\n",
    "    ABA_directory = '/home/zachpen87/clearmap/AtlasDocs/Horizontal'\n",
    "    ABA_ref = 'ABA_25um_reference__1_2_3__slice_None_None_None__slice_None_None_None__slice_None_None_None__.tif'\n",
    "    ref = skimage.io.imread(os.path.join(ABA_directory, ABA_ref))\n",
    "    ref_i = jvis.gen_hmap(ref,plane=plane,title='Allen Brain Atlas',inter=interval)\n",
    "    hmap_i = jvis.gen_hmap(f['_'.join([channel, 'heatmap'])][:],plane=plane,title='Cells',inter=interval,cmap='inferno',lims=(0,.2),alpha=.6,tools=['hover'])\n",
    "\n",
    "(ref_i*hmap_i).opts(title='Cells mapped to Allen Brain Atlas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete intermediate files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View datasets in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdf5_file,'a') as f:\n",
    "    print(f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select datasets to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_dsets = [\n",
    "    'auto',\n",
    "    'fos_647',\n",
    "    'fos_790',\n",
    "    'fos_790_centroids/transformed',\n",
    "    'fos_790_centroids/labels',\n",
    "    'fos_790_heatmap', \n",
    "]\n",
    "\n",
    "with h5py.File(hdf5_file,'a') as f:\n",
    "    for key in kept_dsets:\n",
    "        if key not in f.keys():\n",
    "            print('Warning!!!\\n{key} not found in {file}'.format(key=key, file=hdf5_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rewrite file, saving only chosen datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = os.path.join(directory,'data_temp.hdf5')\n",
    "os.rename(hdf5_file, temp_file)\n",
    "with h5py.File(temp_file,'r') as tempf, h5py.File(hdf5_file,'w') as f:\n",
    "    for key in kept_dsets:\n",
    "        print('writing: {key}'.format(key=key))\n",
    "        f.create_dataset(key, data=tempf[key][:], compression='lzf')\n",
    "os.remove(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clearmap2",
   "language": "python",
   "name": "clearmap2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
